"use strict";(globalThis.webpackChunkwalrus_docusaurus=globalThis.webpackChunkwalrus_docusaurus||[]).push([[1520],{5756(e,n,s){s.d(n,{R:()=>a,x:()=>o});var r=s(9471);let i={},t=r.createContext(i);function a(e){let n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),r.createElement(t.Provider,{value:n},e.children)}},6960(e,n,s){s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>r,toc:()=>c});let r=JSON.parse('{"id":"operator-guide/aggregator","title":"Operating an Aggregator or Publisher","description":"Detailed guide for operating Walrus aggregators and publishers including daemon configuration, metrics, and advanced features.","source":"@site/../content/operator-guide/aggregator.mdx","sourceDirName":"operator-guide","slug":"/operator-guide/aggregator","permalink":"/docs/operator-guide/aggregator","draft":false,"unlisted":false,"editUrl":"https://github.com/MystenLabs/walrus/tree/main/docs/../content/operator-guide/aggregator.mdx","tags":[],"version":"current","frontMatter":{"title":"Operating an Aggregator or Publisher","description":"Detailed guide for operating Walrus aggregators and publishers including daemon configuration, metrics, and advanced features.","keywords":["walrus","aggregator","publisher","daemon","http api","systemd","metrics","large files","operations"]},"sidebar":"operatorSidebar","previous":{"title":"Operator Guide","permalink":"/docs/operator-guide/"},"next":{"title":"The Authenticated Publisher","permalink":"/docs/operator-guide/auth-publisher"}}');var i=s(2615),t=s(5756);let a={title:"Operating an Aggregator or Publisher",description:"Detailed guide for operating Walrus aggregators and publishers including daemon configuration, metrics, and advanced features.",keywords:["walrus","aggregator","publisher","daemon","http api","systemd","metrics","large files","operations"]},o,l={},c=[{value:"Start a local daemon",id:"local-daemon",level:2},{value:"Run an aggregator",id:"run-an-aggregator",level:2},{value:"Sample <code>systemd</code> configuration",id:"sample-systemd-configuration",level:3},{value:"Support large files",id:"support-large-files",level:3},{value:"Run a publisher",id:"run-a-publisher",level:2},{value:"Set sub-wallets and upload concurrency",id:"set-sub-wallets-and-upload-concurrency",level:3},{value:"Manage SUI coins in sub-wallets",id:"manage-sui-coins-in-sub-wallets",level:3},{value:"Understand Blob object lifecycle",id:"understand-blob-object-lifecycle",level:3},{value:"Use authenticated publishers",id:"use-authenticated-publishers",level:3},{value:"Limitations",id:"limitations",level:2},{value:"View daemon metrics",id:"view-daemon-metrics",level:2}];function d(e){let n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",ul:"ul",...(0,t.R)(),...e.components},{Details:s,Term:r}=n;return s||u("Details",!0),r||u("Term",!0),(0,i.jsxs)(i.Fragment,{children:["\n",(0,i.jsxs)(n.p,{children:["Run a Walrus ",(0,i.jsx)(r,{lookup:"Aggregator",children:"aggregator"})," or ",(0,i.jsx)(r,{lookup:"Publisher",children:"publisher"})," to expose the ",(0,i.jsx)(n.a,{href:"/docs/http-api/storing-blobs",children:"HTTP API"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"local-daemon",children:"Start a local daemon"}),"\n",(0,i.jsxs)(n.p,{children:["Run a local Walrus daemon through the ",(0,i.jsx)(n.code,{children:"walrus"})," binary using one of three commands:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"walrus aggregator"}),": Starts an ","aggregator"," that offers an HTTP interface to read ",(0,i.jsx)(r,{lookup:"Blob",children:"blobs"})," from Walrus."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"walrus publisher"}),": Starts a ","publisher"," that offers an HTTP interface to store ","blobs"," in Walrus."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"walrus daemon"}),": Offers the combined functionality of an ","aggregator"," and ","publisher"," on the same address and port."]}),"\n"]}),"\n",(0,i.jsxs)(n.h2,{id:"run-an-aggregator",children:["Run an ","aggregator"]}),"\n",(0,i.jsxs)(n.p,{children:["The ","aggregator"," does not perform any on-chain actions, and only requires specifying the address on which it listens:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sh",children:'$ walrus aggregator --bind-address "127.0.0.1:31415"\n'})}),"\n",(0,i.jsxs)(n.h3,{id:"sample-systemd-configuration",children:["Sample ",(0,i.jsx)(n.code,{children:"systemd"})," configuration"]}),"\n",(0,i.jsxs)(n.p,{children:["The following example shows an ","aggregator"," node that hosts an HTTP endpoint you can use to fetch data from Walrus over the web."]}),"\n",(0,i.jsxs)(n.p,{children:["Run the ","aggregator"," process through the ",(0,i.jsx)(n.code,{children:"walrus"}),(0,i.jsx)(r,{lookup:"Client",children:"client"})," binary using a ",(0,i.jsx)(n.code,{children:"systemd"})," service:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-ini",children:"[Unit]\nDescription=Walrus Aggregator\n\n[Service]\nUser=walrus\nEnvironment=RUST_BACKTRACE=1\nEnvironment=RUST_LOG=info\nExecStart=/opt/walrus/bin/walrus --config /opt/walrus/config/client_config.yaml aggregator --bind-address 0.0.0.0:9000\nRestart=always\n\nLimitNOFILE=65536\n"})}),"\n",(0,i.jsx)(n.h3,{id:"support-large-files",children:"Support large files"}),"\n",(0,i.jsxs)(n.p,{children:["As of Walrus ",(0,i.jsx)(n.code,{children:"v1.38.0"})," the ","aggregator"," can perform concatenation of multiple ","blobs"," through the ",(0,i.jsx)(n.code,{children:"/v1alpha/blobs/concat"})," endpoint. This endpoint enables delivery of very large files, which would otherwise be unsupported due to individual ","blob"," size restrictions. The ",(0,i.jsx)(n.code,{children:"walrus-store-sliced.sh"})," script that follows shows an example of how a very large file can be sliced and uploaded to Walrus. Once the slices are uploaded, the very large file can be read directly by your downstream users by ",(0,i.jsx)(n.code,{children:"GET"})," URL construction (listing the ","blob"," slices in the query params), or through a ",(0,i.jsx)(n.code,{children:"POST"})," request with a JSON body listing the ids. Details of this api are available in the online ","aggregator"," docs. These can be found at ",(0,i.jsx)(n.code,{children:"<your-aggregator-url>/v1/api"}),". This endpoint is still under development and its specifications or behavior might change prior to becoming stable."]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:(0,i.jsx)(n.p,{children:"walrus-store-sliced.sh"})}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sh",children:'#!/bin/bash\n# Copyright (c) Walrus Foundation\n# SPDX-License-Identifier: Apache-2.0\n\nset -euo pipefail\n\nerror() {\n  echo "$0: error: $1" >&2\n}\n\nnote() {\n  echo "$0: note: $1" >&2\n}\n\ndie() {\n  echo "$0: error: $1" >&2\n  exit 1\n}\n\nusage() {\n  echo "Usage: $0 -f <file> -s <size> [-- <walrus store args>...]"\n  echo ""\n  echo "Split a file into chunks and store them using walrus store."\n  echo ""\n  echo "OPTIONS:"\n  echo "  -f <file>             Input file to split (required)"\n  echo "  -s <size>             Chunk size (e.g., 10M, 100K, 1G) (required)"\n  echo "  -h                    Print this usage message"\n  echo "  --                    Delimiter for walrus store arguments"\n  echo ""\n  echo "EXAMPLES:"\n  echo "  $0 -f large_file.txt -s 10M -- --epochs 5"\n  echo "  $0 -f video.mp4 -s 100M -- --epochs max --force"\n  echo ""\n  echo "The chunks will be named: basename_0.ext, basename_1.ext, etc."\n  echo "Chunks are automatically deleted when the script exits."\n}\n\nfile=""\nchunk_size=""\nwalrus_args=()\n\n# Parse arguments\nwhile [[ $# -gt 0 ]]; do\n  case "$1" in\n    -f)\n    file="$2"\n    shift 2\n    ;;\n    -s)\n    chunk_size="$2"\n    shift 2\n    ;;\n    -h)\n    usage\n    exit 0\n    ;;\n    --)\n    shift\n    walrus_args=("$@")\n    break\n    ;;\n    *)\n    error "Unknown option: $1"\n    usage\n    exit 1\n    ;;\n  esac\ndone\n\n# Validate required arguments\nif [[ -z "$file" ]]; then\n  error "input file (-f) is required"\n  usage\n  exit 1\nfi\n\nif [[ -z "$chunk_size" ]]; then\n  error "chunk size (-s) is required"\n  usage\n  exit 1\nfi\n\nif [[ ! -f "$file" ]]; then\n  die "file not found: $file"\nfi\n\n# Extract basename and extension\nfile_basename=$(basename "$file")\nfile_name="${file_basename%.*}"\nfile_ext="${file_basename##*.}"\n\n# Handle case where file has no extension\nif [[ "$file_name" == "$file_ext" ]]; then\n  file_ext=""\nelse\n  file_ext=".$file_ext"\nfi\n\n# Create temp directory for chunks\ntemp_dir=$(mktemp -d -t walrus-chunks-XXXXXX)\ntrap \'rm -rf "\'"$temp_dir" EXIT\nnote "splitting $file into chunks of size $chunk_size in $temp_dir..." >&2\n\n# Split the file into chunks with numeric suffixes\nsplit -b "$chunk_size" "$file" "$temp_dir/chunk_"\n\n# Rename chunks to the desired format: basename_i.ext\nchunk_files=()\ni=0\nfor chunk in "$temp_dir"/chunk_*; do\n  if [[ "$file_ext" == "" ]]; then\n    new_name="$temp_dir/${file_name}_${i}"\n  else\n    new_name="$temp_dir/${file_name}_${i}${file_ext}"\n  fi\n  mv "$chunk" "$new_name"\n  chunk_files+=("$new_name")\n  ((i++))\ndone\n\nnote "created ${#chunk_files[@]} chunks"\n\n# Display the chunks\nfor chunk in "${chunk_files[@]}"; do\n  note "  - $(basename "$chunk")"\ndone\n\n# Call walrus store for each chunk individually\nnote "storing ${#chunk_files[@]} chunks..."\n\nfor chunk_file in "${chunk_files[@]}"; do\n  note "running: walrus store ${walrus_args[*]} $chunk_file"\n\n  if ! walrus store "${walrus_args[@]}" "$chunk_file"; then\n    exit_code=$?\n    error "\u2717 walrus store failed with exit code: $exit_code"\n    note "failed to store entire file. please address issue above and try again."\n    exit $exit_code\n  fi\ndone\n\nnote "\u2713 all chunks stored successfully"\n'})})]}),"\n",(0,i.jsxs)(n.p,{children:["For more information about maximum ","blob"," sizes, run ",(0,i.jsx)(n.code,{children:"walrus info"}),"."]}),"\n",(0,i.jsxs)(n.h2,{id:"run-a-publisher",children:["Run a ","publisher"]}),"\n",(0,i.jsxs)(n.p,{children:["The ","publisher"," and daemon perform on-chain actions and thus require a Sui Testnet wallet with sufficient SUI and ",(0,i.jsx)(r,{lookup:"WAL",children:"WAL"})," balances. To enable handling many parallel requests without object conflicts, they create internal sub-wallets since version 1.4.0, which are funded from the main wallet. These sub-wallets are persisted in a directory specified with the ",(0,i.jsx)(n.code,{children:"--sub-wallets-dir"})," argument. Any existing directory can be used. If it already contains sub-wallets, they are reused."]}),"\n",(0,i.jsxs)(n.p,{children:["By default, 8 sub-wallets are created and funded. You can change this with the ",(0,i.jsx)(n.code,{children:"--n-clients"})," argument. For simple local testing, 1 or 2 sub-wallets are usually sufficient."]}),"\n",(0,i.jsxs)(n.p,{children:["Run a ","publisher"," with a single sub-wallet stored in the Walrus configuration directory:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sh",children:'PUBLISHER_WALLETS_DIR=~/.config/walrus/publisher-wallets\nmkdir -p "$PUBLISHER_WALLETS_DIR"\nwalrus publisher \\\n  --bind-address "127.0.0.1:31416" \\\n  --sub-wallets-dir "$PUBLISHER_WALLETS_DIR" \\\n  --n-clients 1\n'})}),"\n",(0,i.jsxs)(n.p,{children:["Replace ",(0,i.jsx)(n.code,{children:"publisher"})," by ",(0,i.jsx)(n.code,{children:"daemon"})," to run both an ","aggregator"," and ","publisher"," on the same address and port."]}),"\n",(0,i.jsx)(n.admonition,{type:"warning",children:(0,i.jsxs)(n.p,{children:["While the ","aggregator"," does not perform Sui on-chain actions, and therefore consumes no gas, the ","publisher"," does perform actions on-chain and consumes both SUI and ","WAL"," tokens. Ensure only authorized parties can access it, or use other measures to manage gas costs, especially in a future Mainnet deployment."]})}),"\n",(0,i.jsx)(n.h3,{id:"set-sub-wallets-and-upload-concurrency",children:"Set sub-wallets and upload concurrency"}),"\n",(0,i.jsxs)(n.p,{children:["The ","publisher"," uses sub-wallets to allow storing ","blobs"," in parallel. By default, the ","publisher"," uses 8 sub-wallets, meaning it can handle 8 ","blob"," store HTTP requests concurrently."]}),"\n",(0,i.jsxs)(n.p,{children:["To operate a high-performance and concurrency ","publisher",", use the following options:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["The ",(0,i.jsx)(n.code,{children:"--n-clients <NUM>"})," option creates a number of separate wallets used to perform concurrent Sui chain operations. Increase this to allow more parallel uploads. A higher number requires more SUI and ","WAL"," coins initially to be distributed to more wallets."]}),"\n",(0,i.jsxs)(n.li,{children:["The ",(0,i.jsx)(n.code,{children:"--max-concurrent-requests <NUM>"})," determines how many concurrent requests can be handled, including Sui operations (limited by number of clients) but also uploads. After this is exceeded, more requests are queued up to the ",(0,i.jsx)(n.code,{children:"--max-buffer-size <NUM>"}),", after which requests are rejected with an HTTP 429 code."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"manage-sui-coins-in-sub-wallets",children:"Manage SUI coins in sub-wallets"}),"\n",(0,i.jsxs)(n.p,{children:["Each of the sub-wallets requires funds to interact with the chain and purchase storage. A background process checks periodically if the sub-wallets have enough funds. In steady state, each of the sub-wallets has a balance of 0.5-1.0 SUI and ","WAL",". Configure the amount and triggers for coin refills through CLI arguments."]}),"\n",(0,i.jsxs)(n.p,{children:["To tweak how refills are handled, use the ",(0,i.jsx)(n.code,{children:"--refill-interval <REFILL_INTERVAL>"}),", ",(0,i.jsx)(n.code,{children:"--gas-refill-amount <GAS_REFILL_AMOUNT>"}),", ",(0,i.jsx)(n.code,{children:"--wal-refill-amount <WAL_REFILL_AMOUNT>"}),", and ",(0,i.jsx)(n.code,{children:"--sub-wallets-min-balance <SUB_WALLETS_MIN_BALANCE>"})," arguments."]}),"\n",(0,i.jsxs)(n.h3,{id:"understand-blob-object-lifecycle",children:["Understand ","Blob"," object lifecycle"]}),"\n",(0,i.jsxs)(n.p,{children:["Each store operation in Walrus creates a ",(0,i.jsx)(n.code,{children:"Blob"})," object on Sui. This ","blob"," object represents the (partial) ownership over the associated data, and allows certain data management operations (for example, in the case of deletable ","blobs",")."]}),"\n",(0,i.jsxs)(n.p,{children:["When the ","publisher"," stores a ","blob"," on behalf of a ","client",", the ",(0,i.jsx)(n.code,{children:"Blob"})," object is initially owned by the sub-wallet that stored the ","blob",". Then, the following cases are possible, depending on the configuration:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["If the ","client"," requests to store a ","blob"," and specifies the ",(0,i.jsx)(n.code,{children:"send_object_to"})," query parameter (see ",(0,i.jsx)(n.a,{href:"/docs/http-api/storing-blobs#store",children:"the relevant section"})," for examples), then the ",(0,i.jsx)(n.code,{children:"Blob"})," object is transferred to the specified address. This is a way for clients to get back the created object for their data."]}),"\n",(0,i.jsxs)(n.li,{children:["If the ",(0,i.jsx)(n.code,{children:"send_object_to"})," query parameter is not specified, two cases are possible:","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["By default the sub-wallet transfers the newly-created ","blob"," object to the main wallet, such that all these objects are kept there. Change this behavior by setting the ",(0,i.jsx)(n.code,{children:"--burn-after-store"})," flag, and the ","blob"," object is then immediately deleted."]}),"\n",(0,i.jsxs)(n.li,{children:["However, this flag does not affect the use of the ",(0,i.jsx)(n.code,{children:"send_object_to"})," query parameter. Regardless of this flag's status, the ","publisher"," sends created objects to the address in the ",(0,i.jsx)(n.code,{children:"send_object_to"})," query parameter, if it is specified in the PUT request."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"use-authenticated-publishers",children:"Use authenticated publishers"}),"\n",(0,i.jsxs)(n.p,{children:["The setup and use of an authenticated ","publisher"," is covered in a ",(0,i.jsx)(n.a,{href:"/docs/operator-guide/auth-publisher",children:"separate section"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"limitations",children:"Limitations"}),"\n",(0,i.jsxs)(n.p,{children:["By default, ",(0,i.jsx)(n.a,{href:"/docs/http-api/storing-blobs#store",children:"store blob"})," requests are limited to 10 MiB. Increase this limit through the ",(0,i.jsx)(n.code,{children:"--max-body-size"})," option. ",(0,i.jsx)(n.a,{href:"/docs/http-api/storing-blobs#storing-quilts",children:"Store quilt"})," requests are limited to 100 MiB by default, and you can increase them using the ",(0,i.jsx)(n.code,{children:"--max-quilt-body-size"})," option."]}),"\n",(0,i.jsxs)(n.p,{children:["If the ","aggregator"," or ","publisher"," is hosted on a third-party platform, ensure that any additional platform-imposed limits or constraints are compatible with your intended use case."]}),"\n",(0,i.jsx)(n.h2,{id:"view-daemon-metrics",children:"View daemon metrics"}),"\n",(0,i.jsxs)(n.p,{children:["Services by default export a metrics endpoint accessible through ",(0,i.jsx)(n.code,{children:"curl http://127.0.0.1:27182/metrics"}),". Change it using the ",(0,i.jsx)(n.code,{children:"--metrics-address <METRICS_ADDRESS>"})," CLI option."]})]})}function h(e={}){let{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}function u(e,n){throw Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}}}]);