"use strict";(globalThis.webpackChunkwalrus_docusaurus=globalThis.webpackChunkwalrus_docusaurus||[]).push([[4804],{446(e,n,s){s.d(n,{A:()=>t});let t=s.p+"assets/files/walrus_whitepaper_v2-31701a689e75cc47048440b56789f388.pdf"},3395(e,n,s){s.r(n),s.d(n,{assets:()=>a,contentTitle:()=>o,default:()=>h,frontMatter:()=>c,metadata:()=>t,toc:()=>l});let t=JSON.parse('{"id":"design/encoding","title":"Encoding, Overheads, and Verification","description":"Technical details of Walrus encoding using RedStuff erasure codes, data authentication, integrity checks, and consistency verification.","source":"@site/../content/design/encoding.mdx","sourceDirName":"design","slug":"/design/encoding","permalink":"/docs/design/encoding","draft":false,"unlisted":false,"editUrl":"https://github.com/MystenLabs/walrus/tree/main/docs/../content/design/encoding.mdx","tags":[],"version":"current","frontMatter":{"title":"Encoding, Overheads, and Verification","description":"Technical details of Walrus encoding using RedStuff erasure codes, data authentication, integrity checks, and consistency verification.","keywords":["walrus","encoding","erasure codes","redstuff","data integrity","authentication","consistency check","merkle tree","blob verification"]}}');var i=s(2615),r=s(5756);let c={title:"Encoding, Overheads, and Verification",description:"Technical details of Walrus encoding using RedStuff erasure codes, data authentication, integrity checks, and consistency verification.",keywords:["walrus","encoding","erasure codes","redstuff","data integrity","authentication","consistency check","merkle tree","blob verification"]},o,a={},l=[{value:"Basics",id:"basics",level:2},{value:"Erasure coding",id:"erasure-coding",level:3},{value:"Blob authentication",id:"blob-authentication",level:2},{value:"Blob ID computation",id:"blob-id-computation",level:3},{value:"Data integrity and consistency",id:"data-integrity-and-consistency",level:2},{value:"Detection mechanisms",id:"detection-mechanisms",level:3},{value:"Consistency checks",id:"consistency-checks",level:2},{value:"Default consistency check",id:"default-consistency-check",level:3},{value:"Strict consistency check",id:"strict-consistency-check",level:3},{value:"Consistency checks for quilts",id:"consistency-checks-for-quilts",level:3}];function d(e){let n={a:"a",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components},{Term:t}=n;return t||function(e,n){throw Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Term",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(n.p,{children:["Walrus uses a bespoke erasure code construction called ",(0,i.jsx)(t,{lookup:"RedStuff",children:"RedStuff"}),", based on efficiently computable Reed-Solomon codes. For complete technical details, see the ",(0,i.jsx)(n.a,{target:"_blank","data-noBrokenLinkCheck":!0,href:s(446).A+"",children:"Walrus whitepaper"}),". This page summarizes some of the basic techniques and terminology used by Walrus."]}),"\n",(0,i.jsx)(n.h2,{id:"basics",children:"Basics"}),"\n",(0,i.jsxs)(n.p,{children:["Walrus encodes ",(0,i.jsx)(t,{lookup:"Blob",children:"blobs"})," using erasure codes to ensure data can be recovered even when some ",(0,i.jsx)(t,{lookup:"Storage node",children:"storage nodes"})," are unavailable or malicious."]}),"\n",(0,i.jsx)(n.h3,{id:"erasure-coding",children:"Erasure coding"}),"\n",(0,i.jsxs)(n.p,{children:["An ",(0,i.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Erasure_code",children:"erasure code"})," transforms data by:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Taking a ","blob"," and splitting it into (k) symbols."]}),"\n",(0,i.jsx)(n.li,{children:"Encoding these into (n) symbols (where (n > k))."}),"\n",(0,i.jsxs)(n.li,{children:["Distributing symbols so any subset can reconstruct the original ","blob","."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"This process allows Walrus encoding to be:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Highly efficient:"})," Walrus can reconstruct a ","blob"," from just one-third of the encoded symbols."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Systematic:"})," Some ","storage nodes"," hold part of the original ","blob",", enabling fast random-access reads."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Deterministic:"})," All encoding and decoding operations follow fixed algorithms with no discretion."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["For each ","blob",", multiple symbols are combined into a ",(0,i.jsx)(n.strong,{children:(0,i.jsx)(t,{lookup:"Sliver",children:"sliver"})}),", which is then assigned to a ",(0,i.jsx)(t,{lookup:"Shard",children:"shard"}),". ","Storage nodes"," manage one or more shards, and the corresponding slivers of each ","blob"," are distributed to all storage shards."]}),"\n",(0,i.jsxs)(n.p,{children:["This detailed encoding setup results in an expansion of the ","blob"," size by a factor of 4.5-5. This is independent of the number of shards and the number of ","storage nodes","."]}),"\n",(0,i.jsxs)(n.h2,{id:"blob-authentication",children:["Blob"," authentication"]}),"\n",(0,i.jsxs)(n.p,{children:["Each ","blob"," has associated metadata, including a ",(0,i.jsx)(n.strong,{children:(0,i.jsx)(t,{lookup:"Blob ID",children:"blob ID"})}),", that enables data authenticity verification."]}),"\n",(0,i.jsxs)(n.h3,{id:"blob-id-computation",children:["Blob ID"," computation"]}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.strong,{children:"blob ID"})," is computed as an authenticator of the set of all ","shard"," data and metadata (byte size, encoding, ","blob"," hash):"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsxs)(n.strong,{children:["Sliver"," hashing:"]})," Walrus computes a hash of the ","sliver"," representation in each ","shard","."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Merkle tree construction:"})," These hashes form the leaves of a Merkle tree."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsxs)(n.strong,{children:["Blob"," hash derivation:"]})," The Merkle tree root becomes the ","blob"," hash."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Each ","storage node"," can use the ","blob ID"," to authenticate ","shard"," data by checking it against the corresponding ","blob"," hash in the authenticated structure (Merkle tree). A successful check confirms that the data matches what the ","blob"," writer intended."]}),"\n",(0,i.jsx)(n.h2,{id:"data-integrity-and-consistency",children:"Data integrity and consistency"}),"\n",(0,i.jsxs)(n.p,{children:["Clients compute slivers, metadata, and ","blob"," IDs when writing ","blobs",". Because clients are untrusted, these computations might be incorrect due to bugs or malicious intent. Concretely, a ",(0,i.jsx)(t,{lookup:"Client",children:"client"})," can make one or more of the following mistakes:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Incorrect ","sliver"," computation"]}),"\n",(0,i.jsx)(n.li,{children:"Incorrect hash computation from the slivers"}),"\n",(0,i.jsxs)(n.li,{children:["Incorrect ","blob ID"," computation from the ","sliver"," hashes and other metadata"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"detection-mechanisms",children:"Detection mechanisms"}),"\n",(0,i.jsxs)(n.p,{children:["Honest ","storage nodes"," that are assigned an incorrect ","sliver",", or one with an incorrect hash, will attempt to recover it from other ","storage nodes"," and then notice the inconsistency. They can then extract one symbol per ","sliver"," to form an ",(0,i.jsx)(t,{lookup:"Inconsistency proof",children:"inconsistency proof"}),", which is then used to mark the ","blob"," as invalid. After this, ","storage nodes"," can delete slivers belonging to inconsistently encoded ","blobs",", and upon request, return either the ","inconsistency proof"," or an ",(0,i.jsx)(t,{lookup:"Inconsistency certificate",children:"inconsistency certificate"})," posted on-chain."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsxs)(n.strong,{children:["Incorrect ","sliver"," computation:"]})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Can be detected by ","storage nodes"," using the process above."]}),"\n",(0,i.jsx)(n.li,{children:"Not guaranteed to trigger immediately after certification."}),"\n",(0,i.jsx)(n.li,{children:"May persist in the network for longer periods."}),"\n",(0,i.jsxs)(n.li,{children:["Requires additional ","client","-side consistency checks."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Incorrect hash computation:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Generally detected before or shortly after certification using the process above."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsxs)(n.strong,{children:["Incorrect ","blob ID"," computation:"]})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Detected immediately when the ","client"," uploads metadata."]}),"\n",(0,i.jsxs)(n.li,{children:["Storage nodes"," reject these ","blobs"," and never issue storage certificates."]}),"\n",(0,i.jsxs)(n.li,{children:["Such ","blobs"," are never certified."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"consistency-checks",children:"Consistency checks"}),"\n",(0,i.jsxs)(n.p,{children:["Clients perform consistency checks when reading ","blobs"," to ensure data integrity. Walrus offers two levels of verification: ",(0,i.jsx)(n.strong,{children:"default"})," and ",(0,i.jsx)(n.strong,{children:"strict"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["In most cases, the default consistency check is sufficient, particularly when the ","blob"," writer is trusted. The strict consistency check is only needed for specific availability guarantees. When the writer is known and trusted, consistency checks can be disabled entirely."]}),"\n",(0,i.jsx)(n.h3,{id:"default-consistency-check",children:"Default consistency check"}),"\n",(0,i.jsx)(n.p,{children:"The default check balances performance with security by verifying only the read data."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Check process:"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Client"," requests ",(0,i.jsx)(t,{lookup:"Blob metadata",children:"blob metadata"})," and verifies its authenticity."]}),"\n",(0,i.jsxs)(n.li,{children:["Client"," requests a subset of primary slivers (334 slivers minimum)."]}),"\n",(0,i.jsxs)(n.li,{children:["Client"," verifies ","sliver"," authenticity using the metadata."]}),"\n",(0,i.jsxs)(n.li,{children:["Client"," decodes the original ","blob"," from these authentic slivers."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["The first 334 primary slivers contain the unencoded data (potentially padded). This means the cryptographic hashes of these slivers, combined with the ","blob"," length, uniquely determine the ","blob"," content. Therefore, by recomputing the ","sliver"," hashes of the first 334 slivers and checking them against the metadata, the ","client"," can verify that the decoded data is correct. If the hashes match, the data is provided to the ",(0,i.jsx)(t,{lookup:"User",children:"user"}),", otherwise an error is returned."]}),"\n",(0,i.jsxs)(n.p,{children:["Any correct ","client"," attempting to read a ","blob"," and performing the default consistency check, will either read the specific value authenticated by the writer or return an error."]}),"\n",(0,i.jsx)(n.h3,{id:"strict-consistency-check",children:"Strict consistency check"}),"\n",(0,i.jsxs)(n.p,{children:["The strict check provides stronger guarantees by verifying the entire ","blob"," encoding, not just the read portion."]}),"\n",(0,i.jsxs)(n.p,{children:["When a ","blob"," is encoded incorrectly, different sets of 334 slivers might decode to different data. The default check guarantees correct data when a read succeeds, but some read attempts might fail while others succeed. ","Storage nodes"," might later detect the ","blob"," as inconsistent, causing all subsequent reads to fail."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Check process:"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Client"," decodes the ","blob"," (same as default check)."]}),"\n",(0,i.jsxs)(n.li,{children:["Client"," fully re-encodes the decoded ","blob","."]}),"\n",(0,i.jsxs)(n.li,{children:["Client"," recomputes all ","sliver"," hashes and the ","blob ID","."]}),"\n",(0,i.jsxs)(n.li,{children:["Client"," verifies the computed ","blob ID"," matches the requested ","blob ID","."]}),"\n",(0,i.jsxs)(n.li,{children:["Read succeeds only if ","blob"," IDs match."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["This check is more rigorous than the default check and provides the same data consistency property. It ensures the original writer encoded the ","blob"," correctly, eliminating the possibility of inconsistent reads across different clients or time periods."]}),"\n",(0,i.jsxs)(n.p,{children:["Additionally, the strict check guarantees that any correct ","client"," attempting to read the ","blob"," during its lifetime will always succeed and read the same data intended by the writer, regardless of which consistency check they perform."]}),"\n",(0,i.jsx)(n.h3,{id:"consistency-checks-for-quilts",children:"Consistency checks for quilts"}),"\n",(0,i.jsxs)(n.p,{children:["For ",(0,i.jsx)(n.a,{href:"/docs/system-overview/quilt",children:"quilt patches"}),", only a variant of the default consistency check is available, because clients read only part of the quilt rather than the entire structure."]})]})}function h(e={}){let{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},5756(e,n,s){s.d(n,{R:()=>c,x:()=>o});var t=s(9471);let i={},r=t.createContext(i);function c(e){let n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:c(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);